{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "keras.__version__\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.utils import multi_gpu_model\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3\"\n",
    "multi_gpu = len(\"0,1,2,3\".split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session():\n",
    "    config = tf.ConfigProto(device_count = {'CPU' :24, 'GPU':4})\n",
    "    config.gpu_options.allow_growth = True\n",
    "    return tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.tensorflow_backend.set_session(get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Pre-Entrenado CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El directorio de trabajo\n",
    "processDir = './process'\n",
    "\n",
    "# Directorio para entrenamiento, validacion y test\n",
    "train_dir = os.path.join(processDir, 'train')\n",
    "validation_dir = os.path.join(processDir, 'validation')\n",
    "test_dir = os.path.join(processDir, 'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generador de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14530 images belonging to 120 classes.\n",
      "Found 3025 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20, 120)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import InceptionV3\n",
    "\n",
    "if multi_gpu > 1:  \n",
    "    with tf.device('/cpu:0'):\n",
    "        conv_base = InceptionV3(weights='imagenet',\n",
    "                                include_top=False,\n",
    "                                input_shape=(150, 150, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inception_v3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150, 150, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 74, 74, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 74, 74, 32)   96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 74, 74, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 72, 72, 32)   9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 72, 72, 32)   96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 72, 72, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 72, 72, 64)   18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 72, 72, 64)   192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 72, 72, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 35, 35, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 35, 35, 80)   5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 35, 35, 80)   240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 35, 35, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 33, 33, 192)  138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 33, 33, 192)  576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 33, 33, 192)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 64)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 48)   9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 48)   144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 16, 192)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 64)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 96)   288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 16, 16, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 64)   192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 48)   144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 16, 16, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 64)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 96)   288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 16, 16, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 64)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 48)   144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 16, 16, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 64)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 64)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 96)   288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 16, 16, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 64)   192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 7, 7, 384)    995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 7, 7, 96)     82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 7, 7, 384)    1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 7, 7, 96)     288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 7, 7, 384)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 7, 7, 96)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 288)    0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 7, 7, 768)    0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 7, 7, 128)    384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 7, 7, 128)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 7, 7, 128)    114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 7, 7, 128)    384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 7, 7, 128)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 7, 7, 128)    114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 7, 7, 128)    384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 7, 7, 128)    384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 7, 128)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 7, 7, 128)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 7, 7, 128)    114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 7, 7, 128)    114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 7, 7, 128)    384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 7, 7, 128)    384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 7, 7, 128)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 7, 7, 128)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 7, 7, 768)    0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 7, 7, 192)    147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 7, 7, 192)    172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 7, 7, 192)    172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 7, 7, 192)    576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 7, 7, 192)    576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 7, 7, 192)    576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 7, 7, 192)    576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 7, 7, 192)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 7, 7, 192)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 7, 7, 192)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 7, 7, 192)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 7, 7, 768)    0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 7, 7, 160)    480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 160)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 7, 7, 160)    179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 7, 7, 160)    480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 160)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 7, 7, 160)    179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 7, 7, 160)    480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 7, 7, 160)    480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 160)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 160)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 7, 7, 160)    179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 7, 7, 160)    179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 7, 7, 160)    480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 7, 7, 160)    480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 160)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 160)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 7, 7, 768)    0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 7, 7, 192)    147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 7, 7, 192)    215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 7, 7, 192)    215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 7, 7, 192)    576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 7, 7, 192)    576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 7, 7, 192)    576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 7, 7, 192)    576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 192)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 192)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 7, 7, 192)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 7, 7, 192)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 7, 7, 768)    0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 7, 7, 160)    480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 7, 7, 160)    0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 7, 7, 160)    179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 7, 7, 160)    480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 7, 7, 160)    0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 7, 7, 160)    179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 7, 7, 160)    480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 7, 7, 160)    480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 7, 7, 160)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 7, 7, 160)    0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 7, 7, 160)    179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 7, 7, 160)    179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 7, 7, 160)    480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 7, 7, 160)    480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 7, 7, 160)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 7, 7, 160)    0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 7, 7, 768)    0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 7, 7, 192)    147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 7, 7, 192)    215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 7, 7, 192)    215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 7, 7, 192)    576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 7, 7, 192)    576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 7, 7, 192)    576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 7, 7, 192)    576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 7, 7, 192)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 7, 7, 192)    0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 7, 7, 192)    0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 7, 7, 192)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 7, 7, 768)    0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 7, 7, 192)    576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 7, 192)    0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 7, 7, 192)    258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 7, 7, 192)    576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 7, 7, 192)    0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 7, 7, 192)    258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 7, 7, 192)    576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 7, 7, 192)    576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 7, 7, 192)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 7, 7, 192)    0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 7, 7, 192)    258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 7, 7, 192)    258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 7, 7, 192)    576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 7, 7, 192)    576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 7, 7, 192)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 7, 7, 192)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 7, 7, 768)    0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 7, 7, 192)    258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 7, 7, 192)    258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 7, 7, 192)    576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 7, 7, 192)    576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 7, 7, 192)    576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 7, 7, 192)    576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 7, 7, 192)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 7, 7, 192)    0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 7, 7, 192)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 7, 7, 192)    0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 7, 7, 192)    576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 7, 7, 192)    0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 7, 7, 192)    258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 7, 7, 192)    576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 7, 7, 192)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 7, 7, 192)    258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 7, 7, 192)    576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 7, 7, 192)    576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 7, 7, 192)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 7, 7, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 3, 3, 320)    552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 3, 3, 192)    331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 3, 3, 320)    960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 3, 3, 192)    576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 3, 3, 320)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 3, 3, 192)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 3, 3, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 3, 3, 448)    1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 3, 3, 448)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 3, 3, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 3, 3, 384)    1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 3, 3, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 3, 3, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 3, 3, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 3, 3, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 3, 3, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 3, 3, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 3, 3, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 3, 3, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 3, 3, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 3, 3, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 3, 3, 384)    1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 3, 3, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 3, 3, 384)    1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 3, 3, 192)    245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 3, 3, 320)    960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 3, 3, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 3, 3, 384)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 3, 3, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 3, 3, 384)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 3, 3, 192)    576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 3, 3, 320)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 3, 768)    0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 3, 3, 192)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 3, 3, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 3, 3, 448)    1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 3, 3, 448)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 3, 3, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 3, 3, 384)    1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 3, 3, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 3, 3, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 3, 3, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 3, 3, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 3, 3, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 3, 3, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 3, 3, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 3, 3, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 3, 3, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 3, 3, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 3, 3, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 3, 3, 384)    1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 3, 3, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 3, 3, 384)    1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 3, 3, 192)    393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 3, 3, 320)    960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 3, 3, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 3, 3, 384)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 3, 3, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 3, 3, 384)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 3, 3, 192)    576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 3, 3, 320)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3, 3, 768)    0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 3, 3, 192)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(120, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 3, 3, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               4718848   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               30840     \n",
      "=================================================================\n",
      "Total params: 26,552,472\n",
      "Trainable params: 26,518,040\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 3, 3, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               4718848   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               30840     \n",
      "=================================================================\n",
      "Total params: 26,552,472\n",
      "Trainable params: 4,749,688\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_gpu > 1:\n",
    "    parallel_model = multi_gpu_model(model, gpus=multi_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "parallel_model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "                       metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 4.8994 - acc: 0.0175 - val_loss: 5.3246 - val_acc: 0.0210\n",
      "Epoch 2/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 4.8169 - acc: 0.0115 - val_loss: 5.2193 - val_acc: 0.0320\n",
      "Epoch 3/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 4.8029 - acc: 0.0175 - val_loss: 4.9025 - val_acc: 0.0290\n",
      "Epoch 4/500\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 4.7549 - acc: 0.0215 - val_loss: 4.8532 - val_acc: 0.0244\n",
      "Epoch 5/500\n",
      "100/100 [==============================] - 16s 158ms/step - loss: 4.7408 - acc: 0.0170 - val_loss: 5.0609 - val_acc: 0.0570\n",
      "Epoch 6/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 4.7187 - acc: 0.0225 - val_loss: 4.6394 - val_acc: 0.0780\n",
      "Epoch 7/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 4.6810 - acc: 0.0290 - val_loss: 5.4799 - val_acc: 0.0822\n",
      "Epoch 8/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 4.6110 - acc: 0.0390 - val_loss: 5.4420 - val_acc: 0.0840\n",
      "Epoch 9/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 4.5805 - acc: 0.0325 - val_loss: 5.0723 - val_acc: 0.1100\n",
      "Epoch 10/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 4.5301 - acc: 0.0470 - val_loss: 4.8011 - val_acc: 0.1391\n",
      "Epoch 11/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 4.4444 - acc: 0.0560 - val_loss: 5.2358 - val_acc: 0.1650\n",
      "Epoch 12/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 4.4239 - acc: 0.0610 - val_loss: 5.1115 - val_acc: 0.1750\n",
      "Epoch 13/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 4.3818 - acc: 0.0625 - val_loss: 4.9635 - val_acc: 0.1980\n",
      "Epoch 14/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 4.3355 - acc: 0.0740 - val_loss: 4.6455 - val_acc: 0.1970\n",
      "Epoch 15/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 4.2919 - acc: 0.0700 - val_loss: 4.8946 - val_acc: 0.2130\n",
      "Epoch 16/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 4.2677 - acc: 0.0810 - val_loss: 4.9665 - val_acc: 0.2264\n",
      "Epoch 17/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 4.1661 - acc: 0.0985 - val_loss: 4.7704 - val_acc: 0.2410\n",
      "Epoch 18/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 4.1572 - acc: 0.0935 - val_loss: 4.5670 - val_acc: 0.2650\n",
      "Epoch 19/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 4.1786 - acc: 0.0885 - val_loss: 4.9250 - val_acc: 0.2558\n",
      "Epoch 20/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 4.1502 - acc: 0.0870 - val_loss: 4.4045 - val_acc: 0.2980\n",
      "Epoch 21/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 4.0853 - acc: 0.1105 - val_loss: 4.5033 - val_acc: 0.3030\n",
      "Epoch 22/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 4.1062 - acc: 0.0960 - val_loss: 4.8252 - val_acc: 0.2975\n",
      "Epoch 23/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 4.0628 - acc: 0.0940 - val_loss: 4.4655 - val_acc: 0.3030\n",
      "Epoch 24/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 4.0307 - acc: 0.0915 - val_loss: 4.0296 - val_acc: 0.3510\n",
      "Epoch 25/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.9682 - acc: 0.1120 - val_loss: 4.4037 - val_acc: 0.3299\n",
      "Epoch 26/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.9731 - acc: 0.1105 - val_loss: 4.2411 - val_acc: 0.3550\n",
      "Epoch 27/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 4.0158 - acc: 0.1110 - val_loss: 4.3883 - val_acc: 0.3500\n",
      "Epoch 28/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.9540 - acc: 0.1060 - val_loss: 4.2885 - val_acc: 0.3685\n",
      "Epoch 29/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.8603 - acc: 0.1275 - val_loss: 4.5734 - val_acc: 0.3480\n",
      "Epoch 30/500\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 3.9450 - acc: 0.1130 - val_loss: 4.1253 - val_acc: 0.3820\n",
      "Epoch 31/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.8738 - acc: 0.1260 - val_loss: 4.2444 - val_acc: 0.3858\n",
      "Epoch 32/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.8721 - acc: 0.1200 - val_loss: 4.7118 - val_acc: 0.3490\n",
      "Epoch 33/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 3.8453 - acc: 0.1375 - val_loss: 4.1296 - val_acc: 0.3850\n",
      "Epoch 34/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.8441 - acc: 0.1395 - val_loss: 4.1829 - val_acc: 0.3827\n",
      "Epoch 35/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.8732 - acc: 0.1195 - val_loss: 4.4327 - val_acc: 0.3550\n",
      "Epoch 36/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.8251 - acc: 0.1250 - val_loss: 4.3841 - val_acc: 0.3820\n",
      "Epoch 37/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.7965 - acc: 0.1360 - val_loss: 4.3257 - val_acc: 0.3970\n",
      "Epoch 38/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.7730 - acc: 0.1475 - val_loss: 4.1805 - val_acc: 0.4060\n",
      "Epoch 39/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.6900 - acc: 0.1560 - val_loss: 4.4216 - val_acc: 0.3970\n",
      "Epoch 40/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.8454 - acc: 0.1335 - val_loss: 4.7665 - val_acc: 0.3797\n",
      "Epoch 41/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.7602 - acc: 0.1475 - val_loss: 4.2302 - val_acc: 0.3970\n",
      "Epoch 42/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.7947 - acc: 0.1360 - val_loss: 4.4381 - val_acc: 0.4000\n",
      "Epoch 43/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.7728 - acc: 0.1330 - val_loss: 4.3140 - val_acc: 0.3959\n",
      "Epoch 44/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.7221 - acc: 0.1445 - val_loss: 4.2324 - val_acc: 0.4190\n",
      "Epoch 45/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.6801 - acc: 0.1495 - val_loss: 4.4435 - val_acc: 0.4010\n",
      "Epoch 46/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.7086 - acc: 0.1575 - val_loss: 4.2972 - val_acc: 0.4234\n",
      "Epoch 47/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.7457 - acc: 0.1470 - val_loss: 4.5507 - val_acc: 0.3870\n",
      "Epoch 48/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.7372 - acc: 0.1435 - val_loss: 4.0660 - val_acc: 0.4370\n",
      "Epoch 49/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.6448 - acc: 0.1625 - val_loss: 4.5718 - val_acc: 0.4030\n",
      "Epoch 50/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.7081 - acc: 0.1470 - val_loss: 4.6569 - val_acc: 0.4010\n",
      "Epoch 51/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.7485 - acc: 0.1540 - val_loss: 4.1873 - val_acc: 0.4480\n",
      "Epoch 52/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.5888 - acc: 0.1670 - val_loss: 4.1044 - val_acc: 0.4254\n",
      "Epoch 53/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.6307 - acc: 0.1475 - val_loss: 4.3244 - val_acc: 0.4260\n",
      "Epoch 54/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.6431 - acc: 0.1700 - val_loss: 4.1671 - val_acc: 0.4370\n",
      "Epoch 55/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.6611 - acc: 0.1540 - val_loss: 4.6191 - val_acc: 0.4234\n",
      "Epoch 56/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.6488 - acc: 0.1595 - val_loss: 4.7797 - val_acc: 0.3940\n",
      "Epoch 57/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.6454 - acc: 0.1730 - val_loss: 3.8175 - val_acc: 0.4630\n",
      "Epoch 58/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5745 - acc: 0.1765 - val_loss: 4.3582 - val_acc: 0.4416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.5978 - acc: 0.1820 - val_loss: 4.6284 - val_acc: 0.4050\n",
      "Epoch 60/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.6205 - acc: 0.1645 - val_loss: 4.2065 - val_acc: 0.4430\n",
      "Epoch 61/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5949 - acc: 0.1645 - val_loss: 4.2211 - val_acc: 0.4365\n",
      "Epoch 62/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.6387 - acc: 0.1645 - val_loss: 4.0744 - val_acc: 0.4570\n",
      "Epoch 63/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.6096 - acc: 0.1695 - val_loss: 4.2102 - val_acc: 0.4450\n",
      "Epoch 64/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.6362 - acc: 0.1625 - val_loss: 4.4121 - val_acc: 0.4396\n",
      "Epoch 65/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.6155 - acc: 0.1715 - val_loss: 4.8697 - val_acc: 0.4110\n",
      "Epoch 66/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.5693 - acc: 0.1790 - val_loss: 4.3468 - val_acc: 0.4300\n",
      "Epoch 67/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5502 - acc: 0.1825 - val_loss: 4.3541 - val_acc: 0.4467\n",
      "Epoch 68/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5553 - acc: 0.1750 - val_loss: 4.3689 - val_acc: 0.4500\n",
      "Epoch 69/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5867 - acc: 0.1785 - val_loss: 4.4830 - val_acc: 0.4220\n",
      "Epoch 70/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5607 - acc: 0.1805 - val_loss: 4.3409 - val_acc: 0.4497\n",
      "Epoch 71/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.5159 - acc: 0.1770 - val_loss: 4.3201 - val_acc: 0.4470\n",
      "Epoch 72/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5941 - acc: 0.1665 - val_loss: 4.3746 - val_acc: 0.4660\n",
      "Epoch 73/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.5627 - acc: 0.1820 - val_loss: 4.4780 - val_acc: 0.4426\n",
      "Epoch 74/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5487 - acc: 0.1775 - val_loss: 4.6324 - val_acc: 0.4320\n",
      "Epoch 75/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.5310 - acc: 0.1870 - val_loss: 4.5171 - val_acc: 0.4270\n",
      "Epoch 76/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.6045 - acc: 0.1610 - val_loss: 4.1283 - val_acc: 0.4629\n",
      "Epoch 77/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.5253 - acc: 0.1760 - val_loss: 4.6350 - val_acc: 0.4520\n",
      "Epoch 78/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.5951 - acc: 0.1770 - val_loss: 4.2242 - val_acc: 0.4370\n",
      "Epoch 79/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5848 - acc: 0.1750 - val_loss: 4.1683 - val_acc: 0.4550\n",
      "Epoch 80/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5090 - acc: 0.1980 - val_loss: 4.5215 - val_acc: 0.4508\n",
      "Epoch 81/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5379 - acc: 0.1800 - val_loss: 4.3955 - val_acc: 0.4540\n",
      "Epoch 82/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5579 - acc: 0.1835 - val_loss: 4.0643 - val_acc: 0.4650\n",
      "Epoch 83/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4912 - acc: 0.1840 - val_loss: 4.2533 - val_acc: 0.4792\n",
      "Epoch 84/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5881 - acc: 0.1730 - val_loss: 4.3366 - val_acc: 0.4350\n",
      "Epoch 85/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.5048 - acc: 0.1920 - val_loss: 4.7267 - val_acc: 0.4320\n",
      "Epoch 86/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.5306 - acc: 0.1730 - val_loss: 4.9884 - val_acc: 0.4264\n",
      "Epoch 87/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4988 - acc: 0.1780 - val_loss: 4.0712 - val_acc: 0.4730\n",
      "Epoch 88/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4438 - acc: 0.2060 - val_loss: 4.2586 - val_acc: 0.4730\n",
      "Epoch 89/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.5617 - acc: 0.1660 - val_loss: 4.5677 - val_acc: 0.4376\n",
      "Epoch 90/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.5057 - acc: 0.1840 - val_loss: 4.4593 - val_acc: 0.4490\n",
      "Epoch 91/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4872 - acc: 0.1880 - val_loss: 4.3523 - val_acc: 0.4650\n",
      "Epoch 92/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.5259 - acc: 0.1885 - val_loss: 4.3595 - val_acc: 0.4589\n",
      "Epoch 93/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4999 - acc: 0.1875 - val_loss: 4.2736 - val_acc: 0.4830\n",
      "Epoch 94/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.4577 - acc: 0.1905 - val_loss: 4.2226 - val_acc: 0.4760\n",
      "Epoch 95/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4602 - acc: 0.1880 - val_loss: 4.4971 - val_acc: 0.4538\n",
      "Epoch 96/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.4491 - acc: 0.2045 - val_loss: 4.4505 - val_acc: 0.4650\n",
      "Epoch 97/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.5025 - acc: 0.1890 - val_loss: 4.3466 - val_acc: 0.4500\n",
      "Epoch 98/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.4857 - acc: 0.1850 - val_loss: 4.6280 - val_acc: 0.4558\n",
      "Epoch 99/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4968 - acc: 0.1935 - val_loss: 4.1433 - val_acc: 0.4850\n",
      "Epoch 100/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.4463 - acc: 0.1830 - val_loss: 4.0987 - val_acc: 0.4810\n",
      "Epoch 101/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4360 - acc: 0.1955 - val_loss: 4.5046 - val_acc: 0.4548\n",
      "Epoch 102/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.4656 - acc: 0.1915 - val_loss: 4.2565 - val_acc: 0.4740\n",
      "Epoch 103/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4613 - acc: 0.1890 - val_loss: 4.3566 - val_acc: 0.4770\n",
      "Epoch 104/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4740 - acc: 0.1975 - val_loss: 4.3832 - val_acc: 0.4660\n",
      "Epoch 105/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 3.4986 - acc: 0.1870 - val_loss: 4.4483 - val_acc: 0.4500\n",
      "Epoch 106/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4386 - acc: 0.1910 - val_loss: 4.2326 - val_acc: 0.4840\n",
      "Epoch 107/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.4753 - acc: 0.1760 - val_loss: 4.4384 - val_acc: 0.4569\n",
      "Epoch 108/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4674 - acc: 0.1780 - val_loss: 4.5499 - val_acc: 0.4680\n",
      "Epoch 109/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.5032 - acc: 0.1900 - val_loss: 3.9019 - val_acc: 0.5140\n",
      "Epoch 110/500\n",
      "100/100 [==============================] - 17s 166ms/step - loss: 3.5045 - acc: 0.1930 - val_loss: 4.2795 - val_acc: 0.4650\n",
      "Epoch 111/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4756 - acc: 0.1885 - val_loss: 4.1674 - val_acc: 0.4810\n",
      "Epoch 112/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4058 - acc: 0.2000 - val_loss: 4.3838 - val_acc: 0.4800\n",
      "Epoch 113/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4424 - acc: 0.1940 - val_loss: 4.6722 - val_acc: 0.4558\n",
      "Epoch 114/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.4466 - acc: 0.1990 - val_loss: 4.2520 - val_acc: 0.4800\n",
      "Epoch 115/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4046 - acc: 0.2040 - val_loss: 4.5517 - val_acc: 0.4810\n",
      "Epoch 116/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4560 - acc: 0.1915 - val_loss: 4.4383 - val_acc: 0.4640\n",
      "Epoch 117/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.4476 - acc: 0.1945 - val_loss: 4.3776 - val_acc: 0.4730\n",
      "Epoch 118/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3994 - acc: 0.1985 - val_loss: 4.4684 - val_acc: 0.4830\n",
      "Epoch 119/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4667 - acc: 0.1970 - val_loss: 4.2889 - val_acc: 0.4843\n",
      "Epoch 120/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4017 - acc: 0.1960 - val_loss: 4.8120 - val_acc: 0.4440\n",
      "Epoch 121/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.4930 - acc: 0.1885 - val_loss: 4.3369 - val_acc: 0.4780\n",
      "Epoch 122/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4791 - acc: 0.1905 - val_loss: 4.2915 - val_acc: 0.4843\n",
      "Epoch 123/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.5067 - acc: 0.1770 - val_loss: 4.4543 - val_acc: 0.4820\n",
      "Epoch 124/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3969 - acc: 0.2065 - val_loss: 4.1673 - val_acc: 0.4840\n",
      "Epoch 125/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.4231 - acc: 0.1995 - val_loss: 4.3940 - val_acc: 0.4812\n",
      "Epoch 126/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4601 - acc: 0.1980 - val_loss: 4.1643 - val_acc: 0.5000\n",
      "Epoch 127/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3784 - acc: 0.2110 - val_loss: 4.1772 - val_acc: 0.4950\n",
      "Epoch 128/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3938 - acc: 0.2015 - val_loss: 4.4940 - val_acc: 0.4670\n",
      "Epoch 129/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3449 - acc: 0.2130 - val_loss: 4.3658 - val_acc: 0.4870\n",
      "Epoch 130/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4251 - acc: 0.2010 - val_loss: 4.6266 - val_acc: 0.4680\n",
      "Epoch 131/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3995 - acc: 0.2110 - val_loss: 4.5228 - val_acc: 0.4731\n",
      "Epoch 132/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3872 - acc: 0.2060 - val_loss: 4.2933 - val_acc: 0.4880\n",
      "Epoch 133/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4030 - acc: 0.1955 - val_loss: 4.4309 - val_acc: 0.4690\n",
      "Epoch 134/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4847 - acc: 0.1990 - val_loss: 4.4525 - val_acc: 0.4822\n",
      "Epoch 135/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4121 - acc: 0.2040 - val_loss: 4.6892 - val_acc: 0.4580\n",
      "Epoch 136/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3606 - acc: 0.1965 - val_loss: 4.5522 - val_acc: 0.4630\n",
      "Epoch 137/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4045 - acc: 0.1925 - val_loss: 4.1131 - val_acc: 0.4985\n",
      "Epoch 138/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3969 - acc: 0.2155 - val_loss: 4.5964 - val_acc: 0.4560\n",
      "Epoch 139/500\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 3.4071 - acc: 0.2110 - val_loss: 4.7197 - val_acc: 0.4520\n",
      "Epoch 140/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.4151 - acc: 0.2050 - val_loss: 4.0184 - val_acc: 0.4934\n",
      "Epoch 141/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3543 - acc: 0.2110 - val_loss: 4.6928 - val_acc: 0.4620\n",
      "Epoch 142/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.4467 - acc: 0.1905 - val_loss: 4.4289 - val_acc: 0.4740\n",
      "Epoch 143/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4058 - acc: 0.2040 - val_loss: 4.3776 - val_acc: 0.5036\n",
      "Epoch 144/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3840 - acc: 0.2125 - val_loss: 4.5090 - val_acc: 0.4690\n",
      "Epoch 145/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3873 - acc: 0.2115 - val_loss: 4.5433 - val_acc: 0.4620\n",
      "Epoch 146/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4264 - acc: 0.2030 - val_loss: 4.6142 - val_acc: 0.4782\n",
      "Epoch 147/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3875 - acc: 0.2030 - val_loss: 4.5920 - val_acc: 0.4630\n",
      "Epoch 148/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3933 - acc: 0.1915 - val_loss: 4.4241 - val_acc: 0.4850\n",
      "Epoch 149/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3610 - acc: 0.2085 - val_loss: 4.2882 - val_acc: 0.4832\n",
      "Epoch 150/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3604 - acc: 0.2255 - val_loss: 4.5103 - val_acc: 0.4940\n",
      "Epoch 151/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3914 - acc: 0.2085 - val_loss: 4.1847 - val_acc: 0.4890\n",
      "Epoch 152/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4173 - acc: 0.2040 - val_loss: 4.2969 - val_acc: 0.4893\n",
      "Epoch 153/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3986 - acc: 0.1945 - val_loss: 4.5749 - val_acc: 0.4630\n",
      "Epoch 154/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3809 - acc: 0.1860 - val_loss: 4.7873 - val_acc: 0.4480\n",
      "Epoch 155/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3780 - acc: 0.2105 - val_loss: 4.3181 - val_acc: 0.4970\n",
      "Epoch 156/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3803 - acc: 0.2005 - val_loss: 4.5931 - val_acc: 0.4701\n",
      "Epoch 157/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4472 - acc: 0.1910 - val_loss: 4.1215 - val_acc: 0.5050\n",
      "Epoch 158/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3213 - acc: 0.2140 - val_loss: 4.4105 - val_acc: 0.4860\n",
      "Epoch 159/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.3430 - acc: 0.2175 - val_loss: 4.7296 - val_acc: 0.4599\n",
      "Epoch 160/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3806 - acc: 0.2020 - val_loss: 4.6312 - val_acc: 0.4810\n",
      "Epoch 161/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3431 - acc: 0.2230 - val_loss: 4.7025 - val_acc: 0.4590\n",
      "Epoch 162/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3493 - acc: 0.2180 - val_loss: 4.3794 - val_acc: 0.4853\n",
      "Epoch 163/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4051 - acc: 0.2125 - val_loss: 4.7872 - val_acc: 0.4650\n",
      "Epoch 164/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.4040 - acc: 0.2035 - val_loss: 4.8086 - val_acc: 0.4550\n",
      "Epoch 165/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.4108 - acc: 0.2195 - val_loss: 4.7340 - val_acc: 0.4751\n",
      "Epoch 166/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4336 - acc: 0.2000 - val_loss: 4.7316 - val_acc: 0.4660\n",
      "Epoch 167/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.4058 - acc: 0.2150 - val_loss: 4.7709 - val_acc: 0.4660\n",
      "Epoch 168/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4046 - acc: 0.2095 - val_loss: 4.5776 - val_acc: 0.4843\n",
      "Epoch 169/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3172 - acc: 0.2245 - val_loss: 4.1527 - val_acc: 0.4900\n",
      "Epoch 170/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3400 - acc: 0.2200 - val_loss: 4.7675 - val_acc: 0.4700\n",
      "Epoch 171/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3864 - acc: 0.2060 - val_loss: 4.5445 - val_acc: 0.4690\n",
      "Epoch 172/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3822 - acc: 0.2235 - val_loss: 5.0142 - val_acc: 0.4480\n",
      "Epoch 173/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3890 - acc: 0.2030 - val_loss: 4.2026 - val_acc: 0.5000\n",
      "Epoch 174/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3773 - acc: 0.2140 - val_loss: 4.4935 - val_acc: 0.4832\n",
      "Epoch 175/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.4110 - acc: 0.2015 - val_loss: 4.5842 - val_acc: 0.4820\n",
      "Epoch 176/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3300 - acc: 0.2260 - val_loss: 4.5141 - val_acc: 0.4750\n",
      "Epoch 177/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3804 - acc: 0.1965 - val_loss: 4.2097 - val_acc: 0.5046\n",
      "Epoch 178/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3879 - acc: 0.2060 - val_loss: 4.5663 - val_acc: 0.4750\n",
      "Epoch 179/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4138 - acc: 0.2000 - val_loss: 4.9286 - val_acc: 0.4600\n",
      "Epoch 180/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3811 - acc: 0.1990 - val_loss: 5.0111 - val_acc: 0.4741\n",
      "Epoch 181/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3201 - acc: 0.2265 - val_loss: 4.1447 - val_acc: 0.5030\n",
      "Epoch 182/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.2800 - acc: 0.2240 - val_loss: 4.6712 - val_acc: 0.4840\n",
      "Epoch 183/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4025 - acc: 0.2105 - val_loss: 4.7525 - val_acc: 0.4619\n",
      "Epoch 184/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3889 - acc: 0.2165 - val_loss: 4.3671 - val_acc: 0.4910\n",
      "Epoch 185/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3520 - acc: 0.2090 - val_loss: 4.2920 - val_acc: 0.5050\n",
      "Epoch 186/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 3.4039 - acc: 0.1990 - val_loss: 4.5245 - val_acc: 0.4792\n",
      "Epoch 187/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3781 - acc: 0.2135 - val_loss: 4.8051 - val_acc: 0.4810\n",
      "Epoch 188/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.4444 - acc: 0.2020 - val_loss: 4.3757 - val_acc: 0.5060\n",
      "Epoch 189/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3727 - acc: 0.2015 - val_loss: 4.7381 - val_acc: 0.4822\n",
      "Epoch 190/500\n",
      "100/100 [==============================] - 17s 168ms/step - loss: 3.3450 - acc: 0.2180 - val_loss: 4.5583 - val_acc: 0.4600\n",
      "Epoch 191/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3588 - acc: 0.2110 - val_loss: 4.5751 - val_acc: 0.4810\n",
      "Epoch 192/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3874 - acc: 0.2040 - val_loss: 4.5828 - val_acc: 0.4812\n",
      "Epoch 193/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3750 - acc: 0.2140 - val_loss: 4.5172 - val_acc: 0.4860\n",
      "Epoch 194/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4153 - acc: 0.2085 - val_loss: 4.6959 - val_acc: 0.4910\n",
      "Epoch 195/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3954 - acc: 0.1975 - val_loss: 4.0533 - val_acc: 0.5198\n",
      "Epoch 196/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3369 - acc: 0.2210 - val_loss: 4.5510 - val_acc: 0.4820\n",
      "Epoch 197/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3352 - acc: 0.2225 - val_loss: 4.8559 - val_acc: 0.4770\n",
      "Epoch 198/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3116 - acc: 0.2245 - val_loss: 4.7975 - val_acc: 0.4822\n",
      "Epoch 199/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3738 - acc: 0.2165 - val_loss: 4.4013 - val_acc: 0.5010\n",
      "Epoch 200/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2835 - acc: 0.2305 - val_loss: 4.3674 - val_acc: 0.5010\n",
      "Epoch 201/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3625 - acc: 0.2210 - val_loss: 4.5607 - val_acc: 0.4619\n",
      "Epoch 202/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3885 - acc: 0.2035 - val_loss: 4.7405 - val_acc: 0.4830\n",
      "Epoch 203/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3407 - acc: 0.2140 - val_loss: 4.4816 - val_acc: 0.4890\n",
      "Epoch 204/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3300 - acc: 0.2240 - val_loss: 4.6616 - val_acc: 0.4812\n",
      "Epoch 205/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3711 - acc: 0.2135 - val_loss: 4.5889 - val_acc: 0.4810\n",
      "Epoch 206/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3378 - acc: 0.2195 - val_loss: 5.3333 - val_acc: 0.4550\n",
      "Epoch 207/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3041 - acc: 0.2230 - val_loss: 4.4833 - val_acc: 0.4924\n",
      "Epoch 208/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2794 - acc: 0.2275 - val_loss: 4.7759 - val_acc: 0.4930\n",
      "Epoch 209/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.4014 - acc: 0.2015 - val_loss: 4.9302 - val_acc: 0.4750\n",
      "Epoch 210/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3731 - acc: 0.2070 - val_loss: 4.6577 - val_acc: 0.4904\n",
      "Epoch 211/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3328 - acc: 0.2210 - val_loss: 4.4635 - val_acc: 0.5050\n",
      "Epoch 212/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3578 - acc: 0.2095 - val_loss: 4.7831 - val_acc: 0.4850\n",
      "Epoch 213/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3569 - acc: 0.2155 - val_loss: 4.9214 - val_acc: 0.4863\n",
      "Epoch 214/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3510 - acc: 0.2210 - val_loss: 5.0552 - val_acc: 0.4740\n",
      "Epoch 215/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3557 - acc: 0.2140 - val_loss: 4.6785 - val_acc: 0.4850\n",
      "Epoch 216/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3126 - acc: 0.2150 - val_loss: 4.6086 - val_acc: 0.4954\n",
      "Epoch 217/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3141 - acc: 0.2205 - val_loss: 4.5428 - val_acc: 0.5060\n",
      "Epoch 218/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3768 - acc: 0.2105 - val_loss: 4.9250 - val_acc: 0.4840\n",
      "Epoch 219/500\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 3.3507 - acc: 0.2180 - val_loss: 4.1944 - val_acc: 0.4975\n",
      "Epoch 220/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.2728 - acc: 0.2145 - val_loss: 4.7100 - val_acc: 0.4900\n",
      "Epoch 221/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3687 - acc: 0.2205 - val_loss: 5.0148 - val_acc: 0.4480\n",
      "Epoch 222/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2877 - acc: 0.2270 - val_loss: 4.4793 - val_acc: 0.5015\n",
      "Epoch 223/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3193 - acc: 0.2205 - val_loss: 4.5768 - val_acc: 0.4920\n",
      "Epoch 224/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3659 - acc: 0.2190 - val_loss: 4.8052 - val_acc: 0.4940\n",
      "Epoch 225/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3604 - acc: 0.2145 - val_loss: 5.0249 - val_acc: 0.4538\n",
      "Epoch 226/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3562 - acc: 0.2130 - val_loss: 4.5199 - val_acc: 0.4890\n",
      "Epoch 227/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3228 - acc: 0.2175 - val_loss: 4.8593 - val_acc: 0.4920\n",
      "Epoch 228/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2806 - acc: 0.2260 - val_loss: 4.7912 - val_acc: 0.4843\n",
      "Epoch 229/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.2850 - acc: 0.2235 - val_loss: 5.1165 - val_acc: 0.4410\n",
      "Epoch 230/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3654 - acc: 0.2120 - val_loss: 4.4834 - val_acc: 0.5000\n",
      "Epoch 231/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4103 - acc: 0.2150 - val_loss: 4.5163 - val_acc: 0.5050\n",
      "Epoch 232/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3924 - acc: 0.2015 - val_loss: 4.2309 - val_acc: 0.5076\n",
      "Epoch 233/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3880 - acc: 0.2140 - val_loss: 4.9656 - val_acc: 0.4610\n",
      "Epoch 234/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3380 - acc: 0.2065 - val_loss: 4.9379 - val_acc: 0.4470\n",
      "Epoch 235/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3207 - acc: 0.2345 - val_loss: 4.4942 - val_acc: 0.5025\n",
      "Epoch 236/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 16s 157ms/step - loss: 3.3548 - acc: 0.2105 - val_loss: 4.2902 - val_acc: 0.4910\n",
      "Epoch 237/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 3.2954 - acc: 0.2290 - val_loss: 4.5762 - val_acc: 0.4960\n",
      "Epoch 238/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.2820 - acc: 0.2210 - val_loss: 4.6416 - val_acc: 0.4924\n",
      "Epoch 239/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3778 - acc: 0.2060 - val_loss: 4.9075 - val_acc: 0.4770\n",
      "Epoch 240/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3069 - acc: 0.2350 - val_loss: 4.6461 - val_acc: 0.4940\n",
      "Epoch 241/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3280 - acc: 0.2240 - val_loss: 5.0693 - val_acc: 0.4741\n",
      "Epoch 242/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3897 - acc: 0.2230 - val_loss: 4.6236 - val_acc: 0.4900\n",
      "Epoch 243/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3551 - acc: 0.2240 - val_loss: 4.9476 - val_acc: 0.4710\n",
      "Epoch 244/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3236 - acc: 0.2285 - val_loss: 4.7387 - val_acc: 0.4863\n",
      "Epoch 245/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3167 - acc: 0.2040 - val_loss: 4.7485 - val_acc: 0.4890\n",
      "Epoch 246/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3226 - acc: 0.2165 - val_loss: 4.7218 - val_acc: 0.4760\n",
      "Epoch 247/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3639 - acc: 0.2180 - val_loss: 5.1455 - val_acc: 0.4650\n",
      "Epoch 248/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.2814 - acc: 0.2275 - val_loss: 5.2522 - val_acc: 0.4550\n",
      "Epoch 249/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2943 - acc: 0.2260 - val_loss: 4.9550 - val_acc: 0.4860\n",
      "Epoch 250/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3161 - acc: 0.2160 - val_loss: 5.0849 - val_acc: 0.4650\n",
      "Epoch 251/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.3295 - acc: 0.2170 - val_loss: 4.9306 - val_acc: 0.4940\n",
      "Epoch 252/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3314 - acc: 0.2065 - val_loss: 4.5788 - val_acc: 0.4970\n",
      "Epoch 253/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3598 - acc: 0.2065 - val_loss: 4.7055 - val_acc: 0.4975\n",
      "Epoch 254/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3431 - acc: 0.2145 - val_loss: 4.8208 - val_acc: 0.4800\n",
      "Epoch 255/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2908 - acc: 0.2365 - val_loss: 5.0892 - val_acc: 0.4620\n",
      "Epoch 256/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.2750 - acc: 0.2395 - val_loss: 4.7681 - val_acc: 0.4863\n",
      "Epoch 257/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3707 - acc: 0.2130 - val_loss: 4.9903 - val_acc: 0.4750\n",
      "Epoch 258/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3164 - acc: 0.2160 - val_loss: 4.5187 - val_acc: 0.4930\n",
      "Epoch 259/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3229 - acc: 0.2255 - val_loss: 5.0845 - val_acc: 0.4782\n",
      "Epoch 260/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3557 - acc: 0.2170 - val_loss: 5.2903 - val_acc: 0.4530\n",
      "Epoch 261/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3670 - acc: 0.2105 - val_loss: 4.5587 - val_acc: 0.4750\n",
      "Epoch 262/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3392 - acc: 0.2155 - val_loss: 5.1172 - val_acc: 0.4660\n",
      "Epoch 263/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3185 - acc: 0.2185 - val_loss: 5.3865 - val_acc: 0.4510\n",
      "Epoch 264/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2734 - acc: 0.2150 - val_loss: 4.6546 - val_acc: 0.5030\n",
      "Epoch 265/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3135 - acc: 0.2220 - val_loss: 4.8865 - val_acc: 0.5036\n",
      "Epoch 266/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3408 - acc: 0.2290 - val_loss: 5.1953 - val_acc: 0.4550\n",
      "Epoch 267/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3808 - acc: 0.2240 - val_loss: 5.2254 - val_acc: 0.4740\n",
      "Epoch 268/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.2871 - acc: 0.2285 - val_loss: 4.7435 - val_acc: 0.4832\n",
      "Epoch 269/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3432 - acc: 0.2135 - val_loss: 4.8790 - val_acc: 0.4940\n",
      "Epoch 270/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3432 - acc: 0.2175 - val_loss: 4.6645 - val_acc: 0.4700\n",
      "Epoch 271/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2708 - acc: 0.2220 - val_loss: 4.8539 - val_acc: 0.5005\n",
      "Epoch 272/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3122 - acc: 0.2190 - val_loss: 5.0517 - val_acc: 0.4780\n",
      "Epoch 273/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2635 - acc: 0.2175 - val_loss: 5.4704 - val_acc: 0.4570\n",
      "Epoch 274/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3774 - acc: 0.2195 - val_loss: 4.7836 - val_acc: 0.4924\n",
      "Epoch 275/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3742 - acc: 0.2065 - val_loss: 4.8652 - val_acc: 0.4790\n",
      "Epoch 276/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.3884 - acc: 0.2095 - val_loss: 5.1651 - val_acc: 0.4760\n",
      "Epoch 277/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.2439 - acc: 0.2335 - val_loss: 5.2475 - val_acc: 0.4629\n",
      "Epoch 278/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3210 - acc: 0.2250 - val_loss: 5.1539 - val_acc: 0.4550\n",
      "Epoch 279/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3841 - acc: 0.2015 - val_loss: 4.9813 - val_acc: 0.4800\n",
      "Epoch 280/500\n",
      "100/100 [==============================] - 15s 152ms/step - loss: 3.3881 - acc: 0.2065 - val_loss: 5.0629 - val_acc: 0.4721\n",
      "Epoch 281/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3067 - acc: 0.2160 - val_loss: 4.9324 - val_acc: 0.4730\n",
      "Epoch 282/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.3922 - acc: 0.2095 - val_loss: 5.1587 - val_acc: 0.4530\n",
      "Epoch 283/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3205 - acc: 0.2100 - val_loss: 4.8799 - val_acc: 0.5025\n",
      "Epoch 284/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3175 - acc: 0.2135 - val_loss: 4.8317 - val_acc: 0.4880\n",
      "Epoch 285/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.2750 - acc: 0.2240 - val_loss: 5.4671 - val_acc: 0.4600\n",
      "Epoch 286/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.2250 - acc: 0.2405 - val_loss: 5.0421 - val_acc: 0.4802\n",
      "Epoch 287/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3805 - acc: 0.2090 - val_loss: 5.0545 - val_acc: 0.4860\n",
      "Epoch 288/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3355 - acc: 0.2195 - val_loss: 4.8986 - val_acc: 0.4780\n",
      "Epoch 289/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3368 - acc: 0.2140 - val_loss: 5.1045 - val_acc: 0.4569\n",
      "Epoch 290/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3733 - acc: 0.2115 - val_loss: 5.4661 - val_acc: 0.4600\n",
      "Epoch 291/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4021 - acc: 0.2055 - val_loss: 4.8697 - val_acc: 0.5000\n",
      "Epoch 292/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3265 - acc: 0.2160 - val_loss: 5.3003 - val_acc: 0.4782\n",
      "Epoch 293/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.2822 - acc: 0.2180 - val_loss: 5.2160 - val_acc: 0.4580\n",
      "Epoch 294/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3825 - acc: 0.2105 - val_loss: 5.2173 - val_acc: 0.4620\n",
      "Epoch 295/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3575 - acc: 0.2225 - val_loss: 4.6178 - val_acc: 0.4924\n",
      "Epoch 296/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 3.3494 - acc: 0.2115 - val_loss: 5.3343 - val_acc: 0.4610\n",
      "Epoch 297/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3254 - acc: 0.2305 - val_loss: 4.5623 - val_acc: 0.5120\n",
      "Epoch 298/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.3022 - acc: 0.2170 - val_loss: 5.4058 - val_acc: 0.4629\n",
      "Epoch 299/500\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 3.3311 - acc: 0.2220 - val_loss: 5.2237 - val_acc: 0.4770\n",
      "Epoch 300/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3492 - acc: 0.2240 - val_loss: 4.7805 - val_acc: 0.4770\n",
      "Epoch 301/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.2751 - acc: 0.2230 - val_loss: 4.9619 - val_acc: 0.4843\n",
      "Epoch 302/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3193 - acc: 0.2250 - val_loss: 5.5429 - val_acc: 0.4570\n",
      "Epoch 303/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4506 - acc: 0.1960 - val_loss: 4.8977 - val_acc: 0.4970\n",
      "Epoch 304/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3921 - acc: 0.2075 - val_loss: 5.3542 - val_acc: 0.4721\n",
      "Epoch 305/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3555 - acc: 0.2220 - val_loss: 5.1512 - val_acc: 0.4670\n",
      "Epoch 306/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3651 - acc: 0.2220 - val_loss: 5.6720 - val_acc: 0.4570\n",
      "Epoch 307/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3385 - acc: 0.2195 - val_loss: 5.0992 - val_acc: 0.4910\n",
      "Epoch 308/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.2947 - acc: 0.2165 - val_loss: 5.5103 - val_acc: 0.4579\n",
      "Epoch 309/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3310 - acc: 0.2210 - val_loss: 4.7240 - val_acc: 0.4970\n",
      "Epoch 310/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3584 - acc: 0.2185 - val_loss: 5.0038 - val_acc: 0.4680\n",
      "Epoch 311/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3266 - acc: 0.2085 - val_loss: 4.9842 - val_acc: 0.4761\n",
      "Epoch 312/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3156 - acc: 0.2440 - val_loss: 5.2356 - val_acc: 0.4590\n",
      "Epoch 313/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.2779 - acc: 0.2340 - val_loss: 5.1396 - val_acc: 0.4830\n",
      "Epoch 314/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3371 - acc: 0.2185 - val_loss: 5.0600 - val_acc: 0.4792\n",
      "Epoch 315/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3488 - acc: 0.2045 - val_loss: 5.1428 - val_acc: 0.4760\n",
      "Epoch 316/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3600 - acc: 0.2220 - val_loss: 5.2084 - val_acc: 0.4680\n",
      "Epoch 317/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3782 - acc: 0.2100 - val_loss: 5.1765 - val_acc: 0.4670\n",
      "Epoch 318/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3416 - acc: 0.2195 - val_loss: 5.2037 - val_acc: 0.4780\n",
      "Epoch 319/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3578 - acc: 0.2120 - val_loss: 5.5515 - val_acc: 0.4600\n",
      "Epoch 320/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3391 - acc: 0.2225 - val_loss: 5.6823 - val_acc: 0.4467\n",
      "Epoch 321/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2808 - acc: 0.2225 - val_loss: 5.3585 - val_acc: 0.4680\n",
      "Epoch 322/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3032 - acc: 0.2345 - val_loss: 4.8034 - val_acc: 0.5020\n",
      "Epoch 323/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3266 - acc: 0.2185 - val_loss: 5.3448 - val_acc: 0.4690\n",
      "Epoch 324/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3365 - acc: 0.2205 - val_loss: 5.1581 - val_acc: 0.4880\n",
      "Epoch 325/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3521 - acc: 0.2225 - val_loss: 5.4568 - val_acc: 0.4460\n",
      "Epoch 326/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3403 - acc: 0.2240 - val_loss: 5.2539 - val_acc: 0.4701\n",
      "Epoch 327/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3501 - acc: 0.2115 - val_loss: 5.4867 - val_acc: 0.4580\n",
      "Epoch 328/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3596 - acc: 0.2140 - val_loss: 5.1603 - val_acc: 0.4840\n",
      "Epoch 329/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3054 - acc: 0.2230 - val_loss: 4.8877 - val_acc: 0.4934\n",
      "Epoch 330/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3392 - acc: 0.2280 - val_loss: 5.4090 - val_acc: 0.4730\n",
      "Epoch 331/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3452 - acc: 0.2120 - val_loss: 5.1971 - val_acc: 0.4640\n",
      "Epoch 332/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3181 - acc: 0.2255 - val_loss: 5.1077 - val_acc: 0.4690\n",
      "Epoch 333/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3403 - acc: 0.2235 - val_loss: 5.0659 - val_acc: 0.4860\n",
      "Epoch 334/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3741 - acc: 0.2110 - val_loss: 4.8818 - val_acc: 0.5030\n",
      "Epoch 335/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3480 - acc: 0.2180 - val_loss: 5.3788 - val_acc: 0.4711\n",
      "Epoch 336/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3125 - acc: 0.2190 - val_loss: 5.2341 - val_acc: 0.4960\n",
      "Epoch 337/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 3.2779 - acc: 0.2315 - val_loss: 4.9110 - val_acc: 0.4900\n",
      "Epoch 338/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3358 - acc: 0.2180 - val_loss: 5.4319 - val_acc: 0.4599\n",
      "Epoch 339/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3386 - acc: 0.2210 - val_loss: 5.1046 - val_acc: 0.4870\n",
      "Epoch 340/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3189 - acc: 0.2155 - val_loss: 5.3912 - val_acc: 0.4440\n",
      "Epoch 341/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.2919 - acc: 0.2180 - val_loss: 5.1289 - val_acc: 0.4751\n",
      "Epoch 342/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3641 - acc: 0.2180 - val_loss: 5.2100 - val_acc: 0.4940\n",
      "Epoch 343/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3579 - acc: 0.2165 - val_loss: 5.7125 - val_acc: 0.4580\n",
      "Epoch 344/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3396 - acc: 0.2220 - val_loss: 5.4972 - val_acc: 0.4701\n",
      "Epoch 345/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2745 - acc: 0.2335 - val_loss: 5.6787 - val_acc: 0.4630\n",
      "Epoch 346/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3907 - acc: 0.2175 - val_loss: 5.4820 - val_acc: 0.4590\n",
      "Epoch 347/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3063 - acc: 0.2250 - val_loss: 5.2524 - val_acc: 0.5086\n",
      "Epoch 348/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3662 - acc: 0.2200 - val_loss: 6.1012 - val_acc: 0.4260\n",
      "Epoch 349/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3248 - acc: 0.2150 - val_loss: 5.5475 - val_acc: 0.4620\n",
      "Epoch 350/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3593 - acc: 0.2105 - val_loss: 5.0074 - val_acc: 0.4863\n",
      "Epoch 351/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3683 - acc: 0.2125 - val_loss: 5.1359 - val_acc: 0.4860\n",
      "Epoch 352/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2515 - acc: 0.2210 - val_loss: 5.4740 - val_acc: 0.4660\n",
      "Epoch 353/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3550 - acc: 0.2130 - val_loss: 5.5207 - val_acc: 0.4893\n",
      "Epoch 354/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3253 - acc: 0.2200 - val_loss: 5.3617 - val_acc: 0.4810\n",
      "Epoch 355/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3397 - acc: 0.2215 - val_loss: 5.9640 - val_acc: 0.4390\n",
      "Epoch 356/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3787 - acc: 0.2070 - val_loss: 5.6253 - val_acc: 0.4457\n",
      "Epoch 357/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3147 - acc: 0.2230 - val_loss: 5.2761 - val_acc: 0.4720\n",
      "Epoch 358/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3196 - acc: 0.2300 - val_loss: 5.5882 - val_acc: 0.4500\n",
      "Epoch 359/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3251 - acc: 0.2230 - val_loss: 5.1181 - val_acc: 0.4843\n",
      "Epoch 360/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3817 - acc: 0.2070 - val_loss: 4.9890 - val_acc: 0.4920\n",
      "Epoch 361/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3419 - acc: 0.2120 - val_loss: 5.5029 - val_acc: 0.4580\n",
      "Epoch 362/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2866 - acc: 0.2390 - val_loss: 5.5302 - val_acc: 0.4680\n",
      "Epoch 363/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3095 - acc: 0.2195 - val_loss: 5.5745 - val_acc: 0.4500\n",
      "Epoch 364/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3561 - acc: 0.2180 - val_loss: 5.5224 - val_acc: 0.4610\n",
      "Epoch 365/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3910 - acc: 0.2070 - val_loss: 5.4945 - val_acc: 0.4548\n",
      "Epoch 366/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.2601 - acc: 0.2275 - val_loss: 5.3985 - val_acc: 0.4660\n",
      "Epoch 367/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2511 - acc: 0.2470 - val_loss: 5.5906 - val_acc: 0.4620\n",
      "Epoch 368/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4011 - acc: 0.2235 - val_loss: 5.5752 - val_acc: 0.4640\n",
      "Epoch 369/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3449 - acc: 0.2170 - val_loss: 5.4710 - val_acc: 0.4730\n",
      "Epoch 370/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3898 - acc: 0.2190 - val_loss: 5.1991 - val_acc: 0.4830\n",
      "Epoch 371/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3351 - acc: 0.2180 - val_loss: 5.5085 - val_acc: 0.4619\n",
      "Epoch 372/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3621 - acc: 0.2120 - val_loss: 5.1442 - val_acc: 0.4820\n",
      "Epoch 373/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.3100 - acc: 0.2180 - val_loss: 5.2374 - val_acc: 0.4900\n",
      "Epoch 374/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3368 - acc: 0.2210 - val_loss: 5.3353 - val_acc: 0.4792\n",
      "Epoch 375/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3688 - acc: 0.2135 - val_loss: 5.4876 - val_acc: 0.4700\n",
      "Epoch 376/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4731 - acc: 0.2040 - val_loss: 5.0858 - val_acc: 0.4790\n",
      "Epoch 377/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3722 - acc: 0.2215 - val_loss: 5.4041 - val_acc: 0.4772\n",
      "Epoch 378/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2960 - acc: 0.2330 - val_loss: 5.1904 - val_acc: 0.4740\n",
      "Epoch 379/500\n",
      "100/100 [==============================] - 17s 167ms/step - loss: 3.3554 - acc: 0.2135 - val_loss: 5.6536 - val_acc: 0.4570\n",
      "Epoch 380/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.3151 - acc: 0.2240 - val_loss: 5.2898 - val_acc: 0.4731\n",
      "Epoch 381/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.3514 - acc: 0.2100 - val_loss: 5.6653 - val_acc: 0.4650\n",
      "Epoch 382/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2606 - acc: 0.2315 - val_loss: 5.4626 - val_acc: 0.4670\n",
      "Epoch 383/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3792 - acc: 0.2240 - val_loss: 5.6662 - val_acc: 0.4510\n",
      "Epoch 384/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3980 - acc: 0.2085 - val_loss: 5.6754 - val_acc: 0.4477\n",
      "Epoch 385/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3763 - acc: 0.2115 - val_loss: 5.3547 - val_acc: 0.4650\n",
      "Epoch 386/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3413 - acc: 0.2175 - val_loss: 5.3960 - val_acc: 0.4850\n",
      "Epoch 387/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3062 - acc: 0.2270 - val_loss: 5.3257 - val_acc: 0.4711\n",
      "Epoch 388/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4318 - acc: 0.1930 - val_loss: 5.6172 - val_acc: 0.4380\n",
      "Epoch 389/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2833 - acc: 0.2220 - val_loss: 5.4840 - val_acc: 0.4730\n",
      "Epoch 390/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.3274 - acc: 0.2195 - val_loss: 5.4383 - val_acc: 0.4721\n",
      "Epoch 391/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3603 - acc: 0.2185 - val_loss: 5.4966 - val_acc: 0.4550\n",
      "Epoch 392/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2948 - acc: 0.2290 - val_loss: 5.2381 - val_acc: 0.4970\n",
      "Epoch 393/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.2963 - acc: 0.2285 - val_loss: 5.7399 - val_acc: 0.4619\n",
      "Epoch 394/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3336 - acc: 0.2180 - val_loss: 5.1089 - val_acc: 0.4910\n",
      "Epoch 395/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3756 - acc: 0.2260 - val_loss: 5.8549 - val_acc: 0.4340\n",
      "Epoch 396/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4089 - acc: 0.2165 - val_loss: 5.3199 - val_acc: 0.4843\n",
      "Epoch 397/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3714 - acc: 0.2170 - val_loss: 5.6117 - val_acc: 0.4690\n",
      "Epoch 398/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3213 - acc: 0.2220 - val_loss: 5.5153 - val_acc: 0.4860\n",
      "Epoch 399/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.3279 - acc: 0.2230 - val_loss: 5.3495 - val_acc: 0.4629\n",
      "Epoch 400/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3311 - acc: 0.2405 - val_loss: 5.5529 - val_acc: 0.4640\n",
      "Epoch 401/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3427 - acc: 0.2225 - val_loss: 5.4217 - val_acc: 0.4890\n",
      "Epoch 402/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3933 - acc: 0.2055 - val_loss: 5.3419 - val_acc: 0.4751\n",
      "Epoch 403/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.2873 - acc: 0.2185 - val_loss: 5.5175 - val_acc: 0.4850\n",
      "Epoch 404/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3626 - acc: 0.2230 - val_loss: 5.7474 - val_acc: 0.4590\n",
      "Epoch 405/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 3.3565 - acc: 0.2145 - val_loss: 5.5469 - val_acc: 0.4629\n",
      "Epoch 406/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 3.3114 - acc: 0.2245 - val_loss: 5.3970 - val_acc: 0.4790\n",
      "Epoch 407/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3620 - acc: 0.2230 - val_loss: 5.7482 - val_acc: 0.4640\n",
      "Epoch 408/500\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 3.3205 - acc: 0.2340 - val_loss: 5.0386 - val_acc: 0.4873\n",
      "Epoch 409/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3315 - acc: 0.2250 - val_loss: 5.4151 - val_acc: 0.4800\n",
      "Epoch 410/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.2744 - acc: 0.2270 - val_loss: 5.9384 - val_acc: 0.4540\n",
      "Epoch 411/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3955 - acc: 0.2200 - val_loss: 5.3157 - val_acc: 0.4914\n",
      "Epoch 412/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3867 - acc: 0.2145 - val_loss: 6.1180 - val_acc: 0.4410\n",
      "Epoch 413/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3929 - acc: 0.2020 - val_loss: 5.6005 - val_acc: 0.4740\n",
      "Epoch 414/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3389 - acc: 0.2350 - val_loss: 4.8900 - val_acc: 0.4873\n",
      "Epoch 415/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3276 - acc: 0.2190 - val_loss: 5.5193 - val_acc: 0.4810\n",
      "Epoch 416/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3177 - acc: 0.2390 - val_loss: 5.7610 - val_acc: 0.4500\n",
      "Epoch 417/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3757 - acc: 0.2110 - val_loss: 5.4567 - val_acc: 0.4619\n",
      "Epoch 418/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3288 - acc: 0.2320 - val_loss: 5.5898 - val_acc: 0.4600\n",
      "Epoch 419/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3728 - acc: 0.2195 - val_loss: 5.4973 - val_acc: 0.4710\n",
      "Epoch 420/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3290 - acc: 0.2195 - val_loss: 5.2222 - val_acc: 0.4954\n",
      "Epoch 421/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3902 - acc: 0.2220 - val_loss: 5.8640 - val_acc: 0.4600\n",
      "Epoch 422/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4042 - acc: 0.2110 - val_loss: 5.3284 - val_acc: 0.4830\n",
      "Epoch 423/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4053 - acc: 0.2260 - val_loss: 6.0014 - val_acc: 0.4467\n",
      "Epoch 424/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.2882 - acc: 0.2305 - val_loss: 5.9400 - val_acc: 0.4440\n",
      "Epoch 425/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3546 - acc: 0.2260 - val_loss: 5.0640 - val_acc: 0.5080\n",
      "Epoch 426/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3115 - acc: 0.2355 - val_loss: 5.7075 - val_acc: 0.4670\n",
      "Epoch 427/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3643 - acc: 0.2185 - val_loss: 5.0263 - val_acc: 0.4940\n",
      "Epoch 428/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3675 - acc: 0.2195 - val_loss: 6.3236 - val_acc: 0.4340\n",
      "Epoch 429/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3881 - acc: 0.2095 - val_loss: 5.5067 - val_acc: 0.4741\n",
      "Epoch 430/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3274 - acc: 0.2295 - val_loss: 5.7222 - val_acc: 0.4640\n",
      "Epoch 431/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3842 - acc: 0.2130 - val_loss: 5.4343 - val_acc: 0.4820\n",
      "Epoch 432/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3608 - acc: 0.2310 - val_loss: 5.4119 - val_acc: 0.4873\n",
      "Epoch 433/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.2945 - acc: 0.2360 - val_loss: 5.6950 - val_acc: 0.4660\n",
      "Epoch 434/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3053 - acc: 0.2285 - val_loss: 5.9724 - val_acc: 0.4510\n",
      "Epoch 435/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.3316 - acc: 0.2350 - val_loss: 5.8896 - val_acc: 0.4416\n",
      "Epoch 436/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3854 - acc: 0.2280 - val_loss: 5.5178 - val_acc: 0.4870\n",
      "Epoch 437/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3894 - acc: 0.2095 - val_loss: 5.7407 - val_acc: 0.4470\n",
      "Epoch 438/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 3.3251 - acc: 0.2245 - val_loss: 5.8174 - val_acc: 0.4457\n",
      "Epoch 439/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.2742 - acc: 0.2380 - val_loss: 5.6887 - val_acc: 0.4660\n",
      "Epoch 440/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.4002 - acc: 0.2020 - val_loss: 5.3960 - val_acc: 0.4780\n",
      "Epoch 441/500\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 3.2989 - acc: 0.2295 - val_loss: 5.7993 - val_acc: 0.4477\n",
      "Epoch 442/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3412 - acc: 0.2210 - val_loss: 5.5123 - val_acc: 0.4700\n",
      "Epoch 443/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3874 - acc: 0.2095 - val_loss: 5.3125 - val_acc: 0.4880\n",
      "Epoch 444/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3614 - acc: 0.2195 - val_loss: 5.9465 - val_acc: 0.4508\n",
      "Epoch 445/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3976 - acc: 0.2065 - val_loss: 5.6248 - val_acc: 0.4750\n",
      "Epoch 446/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3043 - acc: 0.2285 - val_loss: 6.2743 - val_acc: 0.4380\n",
      "Epoch 447/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3422 - acc: 0.2235 - val_loss: 5.4321 - val_acc: 0.4802\n",
      "Epoch 448/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3656 - acc: 0.2295 - val_loss: 5.7248 - val_acc: 0.4500\n",
      "Epoch 449/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3211 - acc: 0.2245 - val_loss: 5.4919 - val_acc: 0.4810\n",
      "Epoch 450/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4301 - acc: 0.1980 - val_loss: 5.9402 - val_acc: 0.4538\n",
      "Epoch 451/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3731 - acc: 0.2035 - val_loss: 5.5318 - val_acc: 0.4930\n",
      "Epoch 452/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4080 - acc: 0.2230 - val_loss: 5.9747 - val_acc: 0.4750\n",
      "Epoch 453/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3900 - acc: 0.2140 - val_loss: 5.6883 - val_acc: 0.4528\n",
      "Epoch 454/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3154 - acc: 0.2285 - val_loss: 5.9214 - val_acc: 0.4480\n",
      "Epoch 455/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3255 - acc: 0.2230 - val_loss: 5.1512 - val_acc: 0.5070\n",
      "Epoch 456/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3844 - acc: 0.2140 - val_loss: 5.6650 - val_acc: 0.4812\n",
      "Epoch 457/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3193 - acc: 0.2290 - val_loss: 5.6569 - val_acc: 0.4860\n",
      "Epoch 458/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3690 - acc: 0.2105 - val_loss: 5.8240 - val_acc: 0.4740\n",
      "Epoch 459/500\n",
      "100/100 [==============================] - 17s 170ms/step - loss: 3.2838 - acc: 0.2260 - val_loss: 5.5365 - val_acc: 0.4700\n",
      "Epoch 460/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3677 - acc: 0.2235 - val_loss: 5.8396 - val_acc: 0.4741\n",
      "Epoch 461/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3612 - acc: 0.2215 - val_loss: 6.0725 - val_acc: 0.4480\n",
      "Epoch 462/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3234 - acc: 0.2245 - val_loss: 5.0805 - val_acc: 0.5010\n",
      "Epoch 463/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3418 - acc: 0.2155 - val_loss: 5.1613 - val_acc: 0.5036\n",
      "Epoch 464/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3776 - acc: 0.2090 - val_loss: 6.0373 - val_acc: 0.4430\n",
      "Epoch 465/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3842 - acc: 0.2160 - val_loss: 5.6785 - val_acc: 0.4630\n",
      "Epoch 466/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3612 - acc: 0.2195 - val_loss: 5.6037 - val_acc: 0.4589\n",
      "Epoch 467/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.3121 - acc: 0.2205 - val_loss: 5.7013 - val_acc: 0.4650\n",
      "Epoch 468/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3123 - acc: 0.2285 - val_loss: 6.0921 - val_acc: 0.4570\n",
      "Epoch 469/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3595 - acc: 0.2285 - val_loss: 6.1078 - val_acc: 0.4538\n",
      "Epoch 470/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4022 - acc: 0.2170 - val_loss: 5.7350 - val_acc: 0.4530\n",
      "Epoch 471/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3585 - acc: 0.2185 - val_loss: 5.9349 - val_acc: 0.4500\n",
      "Epoch 472/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 16s 157ms/step - loss: 3.3290 - acc: 0.2205 - val_loss: 5.5274 - val_acc: 0.4741\n",
      "Epoch 473/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3563 - acc: 0.2240 - val_loss: 5.6969 - val_acc: 0.4750\n",
      "Epoch 474/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3325 - acc: 0.2230 - val_loss: 5.7121 - val_acc: 0.4740\n",
      "Epoch 475/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.4218 - acc: 0.2075 - val_loss: 5.7301 - val_acc: 0.4599\n",
      "Epoch 476/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3500 - acc: 0.2170 - val_loss: 6.0106 - val_acc: 0.4680\n",
      "Epoch 477/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3246 - acc: 0.2325 - val_loss: 6.2182 - val_acc: 0.4370\n",
      "Epoch 478/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 3.2890 - acc: 0.2215 - val_loss: 5.9575 - val_acc: 0.4569\n",
      "Epoch 479/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3604 - acc: 0.2260 - val_loss: 5.9375 - val_acc: 0.4540\n",
      "Epoch 480/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 3.3786 - acc: 0.2175 - val_loss: 6.0536 - val_acc: 0.4470\n",
      "Epoch 481/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3975 - acc: 0.2065 - val_loss: 6.3330 - val_acc: 0.4538\n",
      "Epoch 482/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4008 - acc: 0.2180 - val_loss: 5.7847 - val_acc: 0.4530\n",
      "Epoch 483/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3761 - acc: 0.2050 - val_loss: 5.7224 - val_acc: 0.4600\n",
      "Epoch 484/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.3518 - acc: 0.2295 - val_loss: 5.9767 - val_acc: 0.4487\n",
      "Epoch 485/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3710 - acc: 0.2205 - val_loss: 5.7299 - val_acc: 0.4680\n",
      "Epoch 486/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4029 - acc: 0.2070 - val_loss: 5.9195 - val_acc: 0.4610\n",
      "Epoch 487/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3447 - acc: 0.2085 - val_loss: 6.0745 - val_acc: 0.4518\n",
      "Epoch 488/500\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 3.4176 - acc: 0.2030 - val_loss: 5.8645 - val_acc: 0.4610\n",
      "Epoch 489/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3386 - acc: 0.2220 - val_loss: 5.9392 - val_acc: 0.4570\n",
      "Epoch 490/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.4009 - acc: 0.2200 - val_loss: 6.1406 - val_acc: 0.4487\n",
      "Epoch 491/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 3.3839 - acc: 0.2115 - val_loss: 6.0225 - val_acc: 0.4460\n",
      "Epoch 492/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3799 - acc: 0.2250 - val_loss: 5.8223 - val_acc: 0.4650\n",
      "Epoch 493/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3746 - acc: 0.2240 - val_loss: 6.4052 - val_acc: 0.4305\n",
      "Epoch 494/500\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 3.4156 - acc: 0.1985 - val_loss: 6.2865 - val_acc: 0.4440\n",
      "Epoch 495/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3604 - acc: 0.2110 - val_loss: 6.2071 - val_acc: 0.4380\n",
      "Epoch 496/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.3872 - acc: 0.2130 - val_loss: 6.3066 - val_acc: 0.4528\n",
      "Epoch 497/500\n",
      "100/100 [==============================] - 15s 154ms/step - loss: 3.2813 - acc: 0.2380 - val_loss: 5.6857 - val_acc: 0.4850\n",
      "Epoch 498/500\n",
      "100/100 [==============================] - 16s 156ms/step - loss: 3.2832 - acc: 0.2300 - val_loss: 6.1484 - val_acc: 0.4470\n",
      "Epoch 499/500\n",
      "100/100 [==============================] - 16s 155ms/step - loss: 3.3924 - acc: 0.2185 - val_loss: 6.1267 - val_acc: 0.4467\n",
      "Epoch 500/500\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 3.3612 - acc: 0.2005 - val_loss: 5.6988 - val_acc: 0.4790\n"
     ]
    }
   ],
   "source": [
    "history = parallel_model.fit_generator(train_generator,\n",
    "                                       steps_per_epoch=100,\n",
    "                                       epochs=500,\n",
    "                                       validation_data=valid_generator,\n",
    "                                       validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Train')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation')\n",
    "plt.title('Accuracy: Train, Validation')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Train')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation')\n",
    "plt.title('Loss: Train, Validation')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_InceptionV3_RMSProp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
